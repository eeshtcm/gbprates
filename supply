Got it â€” hereâ€™s a single, end-to-end Python script that builds a 10y ASW fair-value model from scratch, using:

Target: 10y ASW = 10y SONIA swap âˆ’ 10y gilt yield (bps)

Drivers: 10y yield level, GCâ€“SONIA tightness (SONIA âˆ’ GC, bps), GDP-normalised net duration supply (lagged 1m)

It:

loads raw daily/monthly/quarterly CSVs,

constructs NDS/GDP from DMO issuance, redemptions, and APF sales/purchases (duration-weighted to 10y-equivalents),

builds monthly features,

runs OLS with HAC (Neweyâ€“West), prints RÂ²,

produces fair value, residuals, and a simple residual mean-reversion strategy (PnL in bps) with max drawdown and Sharpe.

File schemas (minimal)

dmo_issuance.csv: date, notional_gbp, bucket

dmo_redemptions.csv: date, notional_gbp, bucket

boe_apf_flows.csv: date, notional_gbp, side, bucket (side in {sale,purchase})

ons_nominal_gdp_quarterly.csv: date, nominal_gdp_gbp (quarter-end)

gc_sonia_daily.csv: date, sonia_ond, gc_repo_ond (daily)

yields_daily.csv: date, gilt_10y_yld, swap_10y_yld (daily)

(optional) asw_daily.csv: date, asw_bp (daily). If absent, the script computes ASW as swap âˆ’ gilt.

The script
"""
ASW Fair-Value Model from Scratch
---------------------------------
Target: 10y ASW (bps) = 10y SONIA swap - 10y gilt yield
Drivers:
  - 10y yield level (bps) [choose gilt or swap]
  - GCâ€“SONIA tightness (bps) = SONIA_overnight - GC_repo_overnight
  - GDP-normalised Net Duration Supply (lagged 1m), built from:
      + DMO issuance (10y-equivalents)
      + APF active sales/purchases (QT/QE) (10y-equivalents)
      - Redemptions (10y-equivalents)

Outputs:
  - OLS(HAC) summary with R^2
  - Fair value, residuals, z-score
  - Residual strategy PnL (bps), Sharpe, hit rate, max drawdown (bps)
  - CSVs with intermediate and final series
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.stats.sandwich_covariance import cov_hac
from pathlib import Path

# =========================
# USER CONFIG
# =========================
# Choose yield driver: 'gilt' or 'swap'
YIELD_DRIVER = 'gilt'
# How to aggregate GCâ€“SONIA within month: 'mean' | 'eom' | 'pct5' (5th percentile = "tightest")
TIGHTNESS_SPEC = 'mean'

# 10y duration used for 10y-equivalent scaling
DUR_10Y = 8.5

# Duration priors by bucket (replace with security-level durations when available)
DUR_PRIORS = {
    "short": 4.0, "medium": 9.0, "long": 18.0,
    "linker_short": 7.0, "linker_long": 20.0
}

# Residual strategy params
Z_ENTRY = 1.0      # enter when |z| >= 1.0
Z_EXIT  = 0.0      # exit when z crosses 0
VOL_WIN = 6        # months for vol estimate
TARGET_RISK = 1.0  # arbitrary unit; sizing âˆ 1/vol
SIZE_CAP = 3.0     # cap risk-parity size
COST_PER_CHANGE_BP = 0.2  # cost per position change (bps)

# OoS settings
OOS_TRAIN_MONTHS = 60  # start OoS after 5y of monthly data
HAC_LAGS = 3           # Newey-West lags (months)

# Input file paths (adjust these)
paths = {
    "issuance": "dmo_issuance.csv",
    "redemptions": "dmo_redemptions.csv",
    "apf": "boe_apf_flows.csv",
    "gdp": "ons_nominal_gdp_quarterly.csv",
    "gcsonia": "gc_sonia_daily.csv",
    "yields": "yields_daily.csv",
    # optional: if missing, ASW is computed as swap - gilt
    "asw_daily": "asw_daily.csv",
}

# =========================
# HELPERS
# =========================
def ensure_bps(series: pd.Series) -> pd.Series:
    """Convert a rate series to basis points.
       Heuristic:
         - If median < 0.5 -> treat as decimal (0.045 = 4.5%) => *1e4
         - Else if median < 50 -> treat as percent (4.5 = 4.5%) => *100
         - Else already in bps
    """
    s = series.astype(float).copy()
    med = s.dropna().abs().median()
    if med < 0.5:
        return s * 1e4
    elif med < 50:
        return s * 100
    return s

def map_duration(bucket: str) -> float:
    b = str(bucket).lower()
    if "link" in b and any(k in b for k in ["ultra", "long", "15", "20", "30"]):
        return DUR_PRIORS["linker_long"]
    if "link" in b:
        return DUR_PRIORS["linker_short"]
    if "short" in b:
        return DUR_PRIORS["short"]
    if "medium" in b or "med" in b:
        return DUR_PRIORS["medium"]
    if "long" in b:
        return DUR_PRIORS["long"]
    return DUR_PRIORS["medium"]

def df_exists(path_key: str) -> bool:
    p = Path(paths[path_key])
    return p.exists() and p.is_file()

# =========================
# 1) BUILD NDS/GDP (monthly)
# =========================
def build_nds_over_gdp_monthly() -> pd.DataFrame:
    # Load flows
    iss = pd.read_csv(paths["issuance"], parse_dates=["date"])
    red = pd.read_csv(paths["redemptions"], parse_dates=["date"])
    apf = pd.read_csv(paths["apf"], parse_dates=["date"])
    for d in (iss, red, apf):
        d["bucket"] = d["bucket"].astype(str)

    # 10y-equivalents
    for d in (iss, red, apf):
        d["dur"] = d["bucket"].apply(map_duration)
        d["ten_eq"] = d["notional_gbp"] * (d["dur"] / DUR_10Y)

    # Signed flows
    iss["signed_10yeq"] = iss["ten_eq"]         # + issuance
    red["signed_10yeq"] = -red["ten_eq"]        # - redemptions
    apf["side"] = apf["side"].str.lower()
    apf["signed_10yeq"] = apf["ten_eq"] * apf["side"].map(
        {"sale": +1, "sell": +1, "purchase": -1, "buy": -1}
    )

    # Monthly aggregation
    iss_m = iss.set_index("date").resample("M")["signed_10yeq"].sum().rename("iss_10yeq")
    red_m = red.set_index("date").resample("M")["signed_10yeq"].sum().rename("red_10yeq")
    apf_m = apf.set_index("date").resample("M")["signed_10yeq"].sum().rename("apf_10yeq")

    flows = pd.concat([iss_m, red_m, apf_m], axis=1).fillna(0.0)
    flows["NDS_10yeq"] = flows.sum(axis=1)

    # GDP (quarterly -> monthly)
    gdp_q = pd.read_csv(paths["gdp"], parse_dates=["date"]).set_index("date").sort_index()
    gdp_m = gdp_q.resample("M").interpolate("linear").ffill().rename(
        columns={"nominal_gdp_gbp": "GDP_nominal"}
    )

    nds = flows.join(gdp_m, how="inner")
    nds["NDS_over_GDP"] = nds["NDS_10yeq"] / nds["GDP_nominal"]
    nds["NDS_over_GDP_l1"] = nds["NDS_over_GDP"].shift(1)

    nds.to_csv("build_nds_over_gdp_monthly.csv")
    return nds[["NDS_over_GDP_l1"]]

# =========================
# 2) GCâ€“SONIA tightness (monthly)
# =========================
def build_gc_sonia_tightness_monthly() -> pd.DataFrame:
    gc = pd.read_csv(paths["gcsonia"], parse_dates=["date"]).set_index("date").sort_index()
    # Convert to bps
    gc["sonia_ond_bp"] = ensure_bps(gc["sonia_ond"])
    gc["gc_repo_ond_bp"] = ensure_bps(gc["gc_repo_ond"])
    # Tightness = SONIA - GC (higher => tighter collateral)
    gc["tightness_bp"] = gc["sonia_ond_bp"] - gc["gc_repo_ond_bp"]

    if TIGHTNESS_SPEC == 'eom':
        gc_m = gc["tightness_bp"].resample("M").last().to_frame()
    elif TIGHTNESS_SPEC == 'pct5':
        gc_m = gc["tightness_bp"].resample("M").quantile(0.05).to_frame()
    else:
        gc_m = gc["tightness_bp"].resample("M").mean().to_frame()

    gc_m.to_csv("build_gc_sonia_tightness_monthly.csv")
    return gc_m

# =========================
# 3) 10y yields & ASW (monthly)
# =========================
def build_yields_and_asw_monthly() -> pd.DataFrame:
    yld = pd.read_csv(paths["yields"], parse_dates=["date"]).set_index("date").sort_index()
    yld["gilt_10y_yld_bp"] = ensure_bps(yld["gilt_10y_yld"])
    yld["swap_10y_yld_bp"] = ensure_bps(yld["swap_10y_yld"])

    # ASW: use provided daily if exists; else compute swap - gilt
    if df_exists("asw_daily"):
        asw = pd.read_csv(paths["asw_daily"], parse_dates=["date"]).set_index("date").sort_index()
        asw_m = asw["asw_bp"].resample("M").mean().to_frame("asw_bp")
    else:
        yld["asw_bp"] = yld["swap_10y_yld_bp"] - yld["gilt_10y_yld_bp"]
        asw_m = yld["asw_bp"].resample("M").mean().to_frame()

    yld_m = yld[["gilt_10y_yld_bp", "swap_10y_yld_bp"]].resample("M").mean()

    # Choose driver
    if YIELD_DRIVER == 'gilt':
        yld_m = yld_m.rename(columns={"gilt_10y_yld_bp":"yld_10y_bp"})[["yld_10y_bp"]]
    else:
        yld_m = yld_m.rename(columns={"swap_10y_yld_bp":"yld_10y_bp"})[["yld_10y_bp"]]

    out = asw_m.join(yld_m, how="inner")
    out.to_csv("build_yields_and_asw_monthly.csv")
    return out

# =========================
# 4) Regression + residual strategy
# =========================
def run_regression_and_strategy(df_monthly: pd.DataFrame) -> pd.DataFrame:
    """
    df_monthly columns:
      - asw_bp
      - yld_10y_bp
      - tightness_bp
      - NDS_over_GDP_l1
    Index: month-end timestamps
    """
    df = df_monthly.dropna().copy()

    y = df["asw_bp"]
    X = sm.add_constant(df[["yld_10y_bp","tightness_bp","NDS_over_GDP_l1"]])

    ols = sm.OLS(y, X).fit()
    cov = cov_hac(ols, maxlags=HAC_LAGS)
    ols_hac = ols.get_robustcov_results(cov_type='HAC', cov_kwds={'maxlags':HAC_LAGS})

    print(ols_hac.summary())
    print("In-sample R^2:", round(ols.rsquared, 3))

    # Fair value & residual
    df["fair_value_bp"]   = ols.predict(X)
    df["resid_bp"]        = df["asw_bp"] - df["fair_value_bp"]

    # Expanding OoS R^2
    oos_preds = []
    idx = df.index
    for i in range(OOS_TRAIN_MONTHS, len(df)):
        y_tr = y.iloc[:i]
        X_tr = X.iloc[:i]
        m = sm.OLS(y_tr, X_tr).fit()
        oos_preds.append(m.predict(X.iloc[i:i+1]).iloc[0])
    df.loc[idx[OOS_TRAIN_MONTHS]:, "fv_oos_bp"] = oos_preds
    df["oos_err2"] = (df["asw_bp"] - df["fv_oos_bp"])**2
    df["ins_err2"] = (df["asw_bp"] - df["fair_value_bp"])**2
    oos_r2 = 1 - df["oos_err2"].sum() / ((df["asw_bp"] - df["asw_bp"].mean())**2).sum()
    print("Out-of-sample R^2 (expanding):", round(oos_r2, 3))

    # Residual z-score
    win = 24
    df["resid_mu"] = df["resid_bp"].rolling(win).mean()
    df["resid_sd"] = df["resid_bp"].rolling(win).std()
    df["zscore"] = (df["resid_bp"] - df["resid_mu"]) / df["resid_sd"]

    # Signal construction
    def signal_from_z(z, pos):
        # enter when |z| >= Z_ENTRY; exit when cross 0
        if pos == 0:
            if z >= Z_ENTRY:  return -1
            if z <= -Z_ENTRY: return +1
            return 0
        else:
            if (pos == +1 and z >= Z_EXIT) or (pos == -1 and z <= -Z_EXIT):
                return 0
            return pos

    df["sig"] = 0
    pos = 0
    for i, z in enumerate(df["zscore"].fillna(0.0)):
        pos = signal_from_z(z, pos)
        df.iloc[i, df.columns.get_loc("sig")] = pos

    # Risk-parity sizing
    df["d_asw"] = df["asw_bp"].diff()
    df["vol_bp"] = df["d_asw"].rolling(VOL_WIN).std()
    df["size"] = (TARGET_RISK / df["vol_bp"]).clip(upper=SIZE_CAP).fillna(0.0)

    # Turnover & costs
    turnover = df["sig"].diff().abs().fillna(0.0)
    df["cost_bp"] = turnover * COST_PER_CHANGE_BP

    # PnL (bps): position at t applies to change t->t+1
    df["pnl_bp"] = (df["sig"].shift(1) * df["size"].shift(1) * df["d_asw"]) - df["cost_bp"]
    df["pnl_bp"] = df["pnl_bp"].fillna(0.0)
    df["cum_pnl_bp"] = df["pnl_bp"].cumsum()

    # Max drawdown (bps)
    cum = df["cum_pnl_bp"]
    peak = cum.cummax()
    draw = peak - cum
    max_dd_bp = float(draw.max())
    print("Max drawdown (bps):", round(max_dd_bp, 1))

    # Sharpe, hit rate
    ann = 12
    mu_m = df["pnl_bp"].mean()
    sd_m = df["pnl_bp"].std()
    sharpe = (mu_m / sd_m) * np.sqrt(ann) if sd_m > 0 else np.nan
    hit = (df["pnl_bp"] > 0).mean()
    print(f"Sharpe (ann.): {sharpe:.2f}, Hit rate: {hit:.2%}, Avg monthly PnL (bp): {mu_m:.2f}")

    df.to_csv("asw_fair_value_and_strategy_output.csv")
    return df

# =========================
# MAIN PIPELINE
# =========================
def main():
    nds = build_nds_over_gdp_monthly()                   # NDS_over_GDP_l1
    tight = build_gc_sonia_tightness_monthly()           # tightness_bp
    y_asw = build_yields_and_asw_monthly()               # asw_bp & yld_10y_bp
    monthly = y_asw.join(tight, how="inner").join(nds, how="inner")
    monthly.to_csv("fv_driver_panel_monthly.csv")
    results = run_regression_and_strategy(monthly)
    print("\nSaved files:")
    print(" - fv_driver_panel_monthly.csv")
    print(" - asw_fair_value_and_strategy_output.csv")
    print(" - build_nds_over_gdp_monthly.csv")
    print(" - build_gc_sonia_tightness_monthly.csv")
    print(" - build_yields_and_asw_monthly.csv")

if __name__ == "__main__":
    main()

How to use

Save the script (e.g., asw_fair_value_from_scratch.py) in a folder with your CSVs.

Update the paths dict if your filenames differ.

Choose YIELD_DRIVER ('gilt' or 'swap') and TIGHTNESS_SPEC ('mean', 'eom', or 'pct5').

Run it. It will emit:

fv_driver_panel_monthly.csv (the monthly dataset used)

asw_fair_value_and_strategy_output.csv (FV, residual, z-score, signals, PnL)

Intermediates for auditing.

If you want me to tailor the bucket mapping, add BoE sale bucket dummies, or switch to weekly frequency, I can fold that straight into this template.

You said:

runs **OLS with HAC (Neweyâ€“West)**, prints **RÂ²**,

what is this? when i sau r^2 i mean from a linear regression
ChatGPT said:

Right â€” no need to over-complicate it. When the trader said â€œrun a regression and check the 
ğ‘…
2
R
2
,â€ they meant exactly this:

Take your dependent variable = the 10-year spread (in practice, the 10y gilt ASW = 10y SONIA swap â€“ 10y gilt yield, in bps).

Regress it linearly on your explanatory variables:

the 10-year yield level (bps)

GCâ€“SONIA tightness (bps)

GDP-normalised net duration supply (lagged 1 period, unitless)

So the regression is just:

ASW
ğ‘¡
10
ğ‘¦
=
ğ›¼
+
ğ›½
1
â‹…
Yield
ğ‘¡
10
ğ‘¦
+
ğ›½
2
â‹…
(
SONIA
âˆ’
GC
)
ğ‘¡
+
ğ›½
3
â‹…
NDS/GDP
ğ‘¡
âˆ’
1
+
ğœ€
ğ‘¡
ASW
t
10y
	â€‹

=Î±+Î²
1
	â€‹

â‹…Yield
t
10y
	â€‹

+Î²
2
	â€‹

â‹…(SONIAâˆ’GC)
t
	â€‹

+Î²
3
	â€‹

â‹…NDS/GDP
tâˆ’1
	â€‹

+Îµ
t
	â€‹


You estimate this with ordinary least squares (OLS).

The regression output gives you:

coefficients 
ğ›½
^
ğ‘–
Î²
^
	â€‹

i
	â€‹

 (with signs/t-stats),

the 
ğ‘…
2
R
2
 (the share of variation in ASW explained by these drivers).

That 
ğ‘…
2
R
2
 is exactly what the trader is asking for â€” â€œdoes this model have explanatory power?â€

Why I mentioned HAC before

In finance time-series, errors are often autocorrelated/heteroskedastic. If you want more robust t-stats, you use Newey-West. But the 
ğ‘…
2
R
2
 itself is just the ordinary regression 
ğ‘…
2
R
2
 â€” no difference.

ğŸ‘‰ Do you want me to show you a minimal Python example with dummy data so you see exactly how to run this regression and read off the 
ğ‘…
2
R
2
? That way youâ€™ll have a clean starting point before wiring in DMO, BoE, and GCâ€“SONIA data.

You said:
ok now just explain everything the model is doing in the most basic terms and ask some questions on it so i look like i made progress
ChatGPT said:

Perfect â€” hereâ€™s how to frame your progress in really simple terms and the kinds of smart questions you can ask back to the trader.

ğŸš¦ What the model is doing (basic terms)

Target (what we want to explain):
The 10-year gilt spread (usually the asset swap spread = 10y SONIA swap â€“ 10y gilt yield). Traders care because it shows whether gilts are cheap/rich relative to swaps.

Drivers (the things we think move it):

10y yield level: the overall level of rates. When yields are high, investor demand and hedging dynamics change.

GCâ€“SONIA (repo tightness): shows how scarce gilts are as collateral. If gilts are in high demand for repo, spreads widen.

Net duration supply, scaled by GDP: the amount of government bond risk the market has to absorb (new issuance, QT sales, minus redemptions). More supply per GDP usually cheapens gilts, widening spreads.

The regression:
We run a simple line fit (OLS regression). It finds the best combination of the three drivers that explains how the 10y spread has actually moved in the past.

The 
ğ‘…
2
R
2
:
This number tells us how much of the variation in spreads is explained by these drivers.

An 
ğ‘…
2
R
2
 close to 1 = very good fit.

An 
ğ‘…
2
R
2
 close to 0 = poor fit.

The residual:
Thatâ€™s the bit the model canâ€™t explain. Traders use it as a signal:

If the spread is wider than the model says (positive residual) â†’ gilts look cheap â†’ maybe a short-term buy.

If the spread is tighter than the model says (negative residual) â†’ gilts look rich â†’ maybe a short-term sell.

â“ Questions you can ask to look like youâ€™ve pushed it forward

On the target variable:

â€œShould I define the 10y spread as a true asset swap spread (including repo), or is swap minus gilt yield enough for this analysis?â€

On GCâ€“SONIA:

â€œDo you prefer I use the monthly average GCâ€“SONIA, or should I focus on the most extreme/tightest day of the month, since specials drive ASWs?â€

On supply:

â€œFor net duration supply, should I weight linkers differently, or just approximate with simple duration buckets?â€

On regression design:

â€œShould I include contemporaneous supply, or lag it by one month to avoid look-ahead bias?â€

â€œAre there other control variables (like curve slope or volatility) that youâ€™d expect to matter?â€

On trading the residual:

â€œWould you size positions on the z-score of the residual, or use it more qualitatively as context?â€

â€œHow big a deviation from fair value would you usually need before putting risk on?â€

ğŸ‘‰ This way, you can go back and say: â€œIâ€™ve set up a regression where ASW is explained by 10y yields, GCâ€“SONIA, and net duration supply/GDP. The idea is that the model gives a fair value and the residual is a trading signal. Iâ€™m now working out whether to use average vs extreme repo spreads, how to bucket linker supply, and how best to treat lags.â€

That shows clear progress and invites the trader into the discussion.
